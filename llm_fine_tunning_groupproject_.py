# -*- coding: utf-8 -*-
"""LLM-Fine-Tunning-groupproject .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nn5qYybQIeKFakCRHn0G8ww6sqpTGWI_

# LLM Fine-Tuning with Alpaca Dataset

*** Install Required Libraries.***
"""

!pip install -q transformers datasets peft accelerate bitsandbytes trl

"""**Importing Libraries.**"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer
from datasets import load_dataset
from peft import get_peft_model, LoraConfig, TaskType
from trl import SFTTrainer

"""**Load the Dataset (Alpaca).**"""

dataset = load_dataset("tatsu-lab/alpaca")

print(dataset["train"][0])

"""# 4: Load Base Model & Tokenizer (FLAN-T5-Small)"""

model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""#5: Preprocess Dataset"""

def preprocess(example):
    prompt = example["instruction"] + "\n" + example["input"]
    inputs = tokenizer(prompt, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    labels = tokenizer(example["output"], truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    return {
        "input_ids": inputs["input_ids"][0],
        "attention_mask": inputs["attention_mask"][0],
        "labels": labels["input_ids"][0]
    }

tokenized_dataset = dataset["train"].map(preprocess)

"""#6: Apply PEFT (LoRA) Configuration"""

peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q", "v"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

"""#Step 7: Training Arguments"""

training_args = TrainingArguments(
    output_dir="./flan-t5-lora-alpaca",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=1,
    num_train_epochs=3,
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,  # Use mixed precision on T4
    report_to="none"
)

"""**Trainer.**"""

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=training_args
)

"""*** Start Training.***"""

trainer.train()

"""**Save Fine-Tuned Mode.**"""

trainer.model.save_pretrained("./flan-t5-lora-alpaca")
tokenizer.save_pretrained("./flan-t5-lora-alpaca")

"""**Evaluation .**"""

test_prompts = [
    "Translate English to French:\n My name is Touseef Ahmed and I am doing LLM-Fine Tunning on Alpaca Data-Set.",
    "What is the capital of Pakistan?",
    "Summarize the following:\nLarge language models are transforming AI development.",
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output_ids = model.generate(**inputs, max_new_tokens=50)
    print(f"\nPrompt: {prompt}")
    print("Response:", tokenizer.decode(output_ids[0], skip_special_tokens=True))

